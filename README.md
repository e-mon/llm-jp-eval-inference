# Offline推論ツール
[ [**English**](./README_en.md) | 日本語 ]

本ディレクトリで次のライブラリを用いた高速なバッチ推論処理を実装を公開します。

- [vLLM](./llm-jp-eval-inference/vllm/)
- [TensorRT-LLM](./llm-jp-eval-inference/trtllm/)
- [Hugging Face Transformers](./llm-jp-eval-inference/transformers) (baseline)

また、[Weights & Biases Run管理ツール](./llm-jp-eval-inference/wandb_run_management)の実装を公開します。
